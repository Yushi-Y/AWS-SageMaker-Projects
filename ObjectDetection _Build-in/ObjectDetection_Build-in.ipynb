{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection - Identifying Bees Using Crowd Sourced Data and Amazon SageMaker \n",
    "\n",
    "Source: Amazon SageMaker: Build an Object Detection Model Using Images Labeled with Ground Truth via AWS Skill Builder \n",
    "\n",
    "### Table of contents\n",
    "1. [Introduction to dataset](#introduction)\n",
    "2. [Labeling with Amazon SageMaker Ground Truth](#groundtruth)\n",
    "3. [Reviewing labeling results](#review)\n",
    "4. [Training an Object Detection model](#training)\n",
    "5. [Review of Training Results](#review_training)\n",
    "6. [Model Tuning](#model_tuning)\n",
    "7. [Cleanup](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction to dataset\n",
    "We will use a dataset from the [inaturalist.org](inaturalist.org) This dataset contains 500 images of bees that have been uploaded by inaturalist users for the purposes of recording the observation and identification. We only used images that their users have licensed under [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/) license. The dataset is in S3 in a single zip archive here: http://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/DIG-TF-200-MLBEES-10-EN/dataset.zip\n",
    "\n",
    "First, download and unzip the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/DIG-TF-200-MLBEES-10-EN/dataset.zip \n",
    "!unzip -qo dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive contains the following structure: 500 `.jpg` image files, a manifest file (to be explained later) and 10 test images in the `test` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -l dataset.zip | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's upload this dataset to your own S3 bucket in preparation for labeling and training using Amazon SageMaker. For this demo, we will be using `us-west-2` region, so your bucket needs to be in this region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket must be created in us-west-2 (Oregon) region\n",
    "BUCKET = 'denisb-sagemaker-oregon'\n",
    "PREFIX = 'input' # this is the root path to your working space, feel to use a different path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --exclude=\"*\" --include=\"[0-9]*.jpg\" . s3://$BUCKET/$PREFIX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling with SageMaker Ground Truth <a name=\"groundtruth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to run your first labeling with Amazon SageMaker Ground Truth. Follow the steps shown in the recording.\n",
    "\n",
    "When specifying information needed to configure the labeling UI tool, use the following information:\n",
    "\n",
    "- Brief task description: _\"Draw a bounding box around the bee in this image.\"_\n",
    "- Labels: _\"bee\"_\n",
    "- Good example description: _\"bounding box includes all visible parts of the insect - legs, antennae, etc.\"_\n",
    "- Good example image: https://s3.us-west-2.amazonaws.com/aws-tc-largeobjects/DIG-TF-200-MLBEES-10-EN/bee-good-5535715.jpg\n",
    "- Bad example description: _\"bounding box is too big and/or excludes some visible parts of the insect\"_\n",
    "- Bad example image: https://s3.us-west-2.amazonaws.com/aws-tc-largeobjects/DIG-TF-200-MLBEES-10-EN/bee-bad-5535715.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing labeling results\n",
    "<a name=\"reviewing\"></a>\n",
    "\n",
    "After the labeling job has completed, we can see the results of image annotations right in the SageMaker console itself. The console displays each image as well as the bounding boxes around the bees that were drawn by human labelers.\n",
    "\n",
    "At the same time we can examine the results in the so-called augmented manifest file that was generated. Let's download and examine the manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of the labeling job\n",
    "labeling_job_name = 'bees'\n",
    "\n",
    "# Import the Boto3 library for interacting with AWS services\n",
    "import boto3\n",
    "client = boto3.client('sagemaker')  # Create a client for Amazon SageMaker\n",
    "\n",
    "# Get the S3 output path of the labeling job using its name\n",
    "s3_output = client.describe_labeling_job(LabelingJobName=labeling_job_name)['OutputConfig']['S3OutputPath'] + labeling_job_name\n",
    "\n",
    "# Create the URL for the augmented manifest file that contains the labeling results\n",
    "augmented_manifest_url = f'{s3_output}/manifests/output/output.manifest'\n",
    "\n",
    "# Import the os and shutil libraries for working with files and directories\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Try to create a directory named 'od_output_data/'\n",
    "# If it already exists, remove it and its contents\n",
    "try:\n",
    "    os.makedirs('od_output_data/', exist_ok=False)\n",
    "except FileExistsError:\n",
    "    shutil.rmtree('od_output_data/')\n",
    "\n",
    "# Use the AWS CLI command to copy the augmented manifest file from S3 to the local 'od_output_data/' directory\n",
    "!aws s3 cp $augmented_manifest_url od_output_data/\n",
    "\n",
    "# Define the path to the downloaded augmented manifest file\n",
    "augmented_manifest_file = 'od_output_data/output.manifest'\n",
    "\n",
    "# Display the first 3 lines of the augmented manifest file using the 'head' command\n",
    "!head -3 $augmented_manifest_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot all the annotated images. First, let's define a function that displays the local image file and draws over it the bounding boxes obtained via labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "# Define a function to display an image with annotated bounding boxes\n",
    "def show_annotated_image(img_path, bboxes):\n",
    "    # Load the image from the specified path as a numpy array\n",
    "    im = np.array(Image.open(img_path), dtype=np.uint8)\n",
    "    \n",
    "    # Create figure and axes for displaying the image\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image on the axes\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # Define a cycle of colors for bounding box edges\n",
    "    colors = cycle(['r', 'g', 'b', 'y', 'c', 'm', 'k', 'w'])\n",
    "    \n",
    "    # Loop through each bounding box in the list\n",
    "    for bbox in bboxes:\n",
    "        # Create a Rectangle patch representing the bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (bbox['left'], bbox['top']),\n",
    "            bbox['width'],\n",
    "            bbox['height'],\n",
    "            linewidth=1,\n",
    "            edgecolor=next(colors),  # Get the next color from the cycle\n",
    "            facecolor='none'\n",
    "        )\n",
    "\n",
    "        # Add the rectangle patch to the axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Show the image with bounding boxes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read the augmented manifest (JSON lines format) line by line and display the first 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install jsonlines\n",
    "import jsonlines\n",
    "from itertools import islice\n",
    "\n",
    "# Open the augmented manifest file using jsonlines and read its contents\n",
    "with jsonlines.open(augmented_manifest_file, 'r') as reader:\n",
    "    # Read and process the first 10 descriptions from the manifest\n",
    "    for desc in islice(reader, 10):\n",
    "        # Get the image URL from the description\n",
    "        img_url = desc['source-ref']\n",
    "        \n",
    "        # Extract the image file name from the URL\n",
    "        img_file = os.path.basename(img_url)\n",
    "        \n",
    "        # Check if the image file exists locally\n",
    "        file_exists = os.path.isfile(img_file)\n",
    "\n",
    "        # Get the bounding boxes annotations from the labeling job output\n",
    "        bboxes = desc[labeling_job_name]['annotations']\n",
    "        \n",
    "        # Call the function to display the annotated image\n",
    "        show_annotated_image(img_file, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='training'></a>\n",
    "## Training an Object Detection Model\n",
    "We are now ready to use the labeled dataset in order to train a Machine Learning model using the SageMaker [built-in Object Detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html).\n",
    "\n",
    "For this, we would need to split the full labeled dataset into a training and a validation datasets. Out of the total of 500 images we are going to use 400 for training and 100 for validation. The algorithm will use the first one to train the model and the latter to estimate the accuracy of the model, trained so far. The augmented manifest file from the previously run full labeling job was included in the original zip archive as `output.manifest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the JSONlines augmented manifest file locally (contained in the zip file) and read its contents\n",
    "with jsonlines.open('output.manifest', 'r') as reader:\n",
    "    lines = list(reader)\n",
    "    # Shuffle data in place.\n",
    "    np.random.shuffle(lines)\n",
    "    \n",
    "dataset_size = len(lines)\n",
    "num_training_samples = round(dataset_size*0.8)\n",
    "\n",
    "train_data = lines[:num_training_samples]\n",
    "validation_data = lines[num_training_samples:]\n",
    "\n",
    "augmented_manifest_filename_train = 'train.manifest'\n",
    "\n",
    "# Write the shuffled training data to the new training manifest file\n",
    "with open(augmented_manifest_filename_train, 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "\n",
    "augmented_manifest_filename_validation = 'validation.manifest'\n",
    "\n",
    "# Write the shuffled validation data to the new validation manifest file\n",
    "with open(augmented_manifest_filename_validation, 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        \n",
    "print(f'training samples: {num_training_samples}, validation samples: {len(lines)-num_training_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's upload the two manifest files to S3 in preparation for training. We will use the same bucket you created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If PREFIX is defined, concatenate it with '/training', otherwise use 'training'\n",
    "pfx_training = PREFIX + '/training' if PREFIX else 'training'\n",
    "\n",
    "# Define paths for training and validation data using S3 bucket and prefixes\n",
    "s3_train_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_train)\n",
    "s3_validation_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_validation)\n",
    "\n",
    "# Use the AWS CLI to copy the 'train.manifest' file to the specified S3 location\n",
    "!aws s3 cp train.manifest s3://$BUCKET/$pfx_training/\n",
    "\n",
    "# Use the AWS CLI to copy the 'validation.manifest' file to the specified S3 location\n",
    "!aws s3 cp validation.manifest s3://$BUCKET/$pfx_training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to kick off the training. We will do it from the SageMaker console, but alternatively, you can just run this code in a new cell using SageMaker Python SDK:\n",
    "### Code option\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "\n",
    "# Get the IAM role used for the SageMaker training job\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Create a SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# Get the Amazon SageMaker object detection algorithm image URI\n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(\n",
    "    boto3.Session().region_name, 'object-detection', repo_version='latest')\n",
    "\n",
    "# Define the S3 path for the output of the training job\n",
    "s3_output_path = 's3://{}/{}/output'.format(BUCKET, pfx_training)\n",
    "\n",
    "# Create a unique training job name\n",
    "training_job_name = 'bees-detection-resnet'\n",
    "\n",
    "# Define parameters for the SageMaker training job\n",
    "training_params = {\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,  # Amazon SageMaker object detection image\n",
    "        \"TrainingInputMode\": \"Pipe\"  # Use Pipe mode for training data input\n",
    "    },\n",
    "    \"RoleArn\": role,  # IAM role for the training job\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": s3_output_path  # S3 location for training output\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p2.xlarge\",  # SageMaker instance type\n",
    "        \"VolumeSizeInGB\": 50  # EBS volume size\n",
    "    },\n",
    "    \"TrainingJobName\": training_job_name,\n",
    "    \"HyperParameters\": {\n",
    "        # Define hyperparameters for the object detection algorithm\n",
    "        # These parameters are specific to the algorithm and the task\n",
    "        \"base_network\": \"resnet-50\",\n",
    "        \"use_pretrained_model\": \"1\",\n",
    "        # Other hyperparameters...\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400  # Maximum runtime for the training job\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": s3_train_data_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": ['source-ref', 'bees-500']\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": s3_validation_data_path,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"AttributeNames\": ['source-ref', 'bees-500']\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"RecordWrapperType\": \"RecordIO\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a SageMaker client\n",
    "client = boto3.client(service_name='sagemaker')\n",
    "\n",
    "# Create the SageMaker training job using the defined parameters\n",
    "client.create_training_job(**training_params)\n",
    "\n",
    "# Check the status of the training job\n",
    "status = client.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the progess of the training job, you can refresh the console or repeatedly evaluate the following cell. When the training job status reads `'Completed'`, move on to the next part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### REPLACE WITH YOUR OWN TRAINING JOB NAME\n",
    "# In the above console screenshots the job name was 'bees-detection-resnet'.\n",
    "# But if you used Python to kick off the training job,\n",
    "# then 'training_job_name' is already set, so you can comment out the line below.\n",
    "training_job_name = 'bees-training'\n",
    "##### REPLACE WITH YOUR OWN TRAINING JOB NAME\n",
    "\n",
    "training_info = client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "print(\"Training job status: \", training_info['TrainingJobStatus'])\n",
    "print(\"Secondary status: \", training_info['SecondaryStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='review_training'></a>\n",
    "\n",
    "## Review of Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the SageMaker model out of model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get the current timestamp in GMT format\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "# Construct a unique model name by combining training job name and timestamp\n",
    "model_name = training_job_name + '-model' + timestamp\n",
    "\n",
    "# Get the training image used for the training job\n",
    "training_image = training_info['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "# Get the S3 path to the trained model artifacts\n",
    "model_data = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "# Define the primary container configuration for the SageMaker model\n",
    "primary_container = {\n",
    "    'Image': training_image,  # Use the same training image for inference\n",
    "    'ModelDataUrl': model_data,  # Use the S3 location of the trained model\n",
    "}\n",
    "\n",
    "# Get the IAM role used for executing SageMaker operations\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Create the SageMaker model using the specified configuration\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name,           # Unique name for the model\n",
    "    ExecutionRoleArn=role,          # IAM role for model execution\n",
    "    PrimaryContainer=primary_container)  # Configuration for the primary container\n",
    "\n",
    "# Print the Amazon Resource Name (ARN) of the created model\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get the current timestamp in GMT format\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "# Construct a unique endpoint configuration name by combining training job name and timestamp\n",
    "endpoint_config_name = training_job_name + '-epc' + timestamp\n",
    "\n",
    "# Create the endpoint configuration using the specified configuration\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,  # Unique name for the endpoint configuration\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': 'ml.t2.medium',  # SageMaker instance type for the endpoint\n",
    "        'InitialInstanceCount': 1,       # Number of initial instances\n",
    "        'ModelName': model_name,         # The SageMaker model to be used\n",
    "        'VariantName': 'AllTraffic'      # Variant name for traffic routing\n",
    "    }])\n",
    "\n",
    "# Print the created endpoint configuration details\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "The next cell creates an endpoint that can be validated and incorporated into production applications. This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get the current timestamp in GMT format\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "\n",
    "# Construct a unique endpoint name by combining training job name and timestamp\n",
    "endpoint_name = training_job_name + '-ep' + timestamp\n",
    "\n",
    "# Print the created endpoint name\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "# Define parameters for creating the SageMaker endpoint\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,             # Unique name for the endpoint\n",
    "    'EndpointConfigName': endpoint_config_name # Name of the endpoint configuration\n",
    "}\n",
    "\n",
    "# Create the SageMaker endpoint using the specified parameters\n",
    "endpoint_response = client.create_endpoint(**endpoint_params)\n",
    "\n",
    "# Print the Amazon Resource Name (ARN) of the created endpoint\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name=\"test-tuning-job-008-9ff8af52-ep-2019-07-19-12-25-46\"\n",
    "# get the status of the endpoint\n",
    "response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will invoke the deployed endpoint to detect bees in the 10 test images that were inside the `test` folder in `dataset.zip` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get a list of paths for test images in the 'test' directory\n",
    "test_images = glob.glob('test/*')\n",
    "print(*test_images, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that converts the prediction array returned by our endpoint to the bounding box structure expected by our image display function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert prediction into bounding box data\n",
    "def prediction_to_bbox_data(image_path, prediction):\n",
    "    class_id, confidence, xmin, ymin, xmax, ymax = prediction\n",
    "    width, height = Image.open(image_path).size\n",
    "    bbox_data = {'class_id': class_id,\n",
    "               'height': (ymax-ymin)*height,\n",
    "               'width': (xmax-xmin)*width,\n",
    "               'left': xmin*width,\n",
    "               'top': ymin*height}\n",
    "    return bbox_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each of the test images, the following cell transforms the image into the appropriate format for realtime prediction, repeatedly calls the endpoint, receives back the prediction, and displays the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Function to get predictions for an image using the SageMaker endpoint\n",
    "def get_predictions_for_img(runtime_client, endpoint_name, img_path):\n",
    "    with open(img_path, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    # Invoke the SageMaker endpoint and get the response\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-image',\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    # Read and parse the result from the response\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)\n",
    "    return result\n",
    "\n",
    "# Wait until the endpoint is in the 'InService' status\n",
    "client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')\n",
    "\n",
    "# Loop through each test image and display predicted bounding boxes\n",
    "for test_image in test_images:\n",
    "    result = get_predictions_for_img(runtime_client, endpoint_name, test_image)\n",
    "    confidence_threshold = 0.2  # Minimum confidence threshold for predictions\n",
    "    best_n = 3  # Display the top 'best_n' predictions\n",
    "    # Filter and sort predictions based on confidence\n",
    "    predictions = [prediction for prediction in result['prediction'] if prediction[1] > confidence_threshold]\n",
    "    predictions.sort(reverse=True, key=lambda x: x[1])\n",
    "    # Convert predictions to bounding box data\n",
    "    bboxes = [prediction_to_bbox_data(test_image, prediction) for prediction in predictions[:best_n]]\n",
    "    # Display the annotated image with bounding boxes\n",
    "    show_annotated_image(test_image, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='model_tuning'></a>\n",
    "## Model Tuning\n",
    "\n",
    "When you configured the training job you needed to add many hyperparameters that affect the performance of the algorithm and the quality of the resulting model. \n",
    "\n",
    "If you have experience with the specific algorithm and understand the innerworkings of it, you may already have a good sense of appropriate values. But even then, it's impossible to know the exact best value of each hyperparameter. Often you can zero in on the best values by trying many different combination of values, effectively searching in the hyperparameter space. SageMaker makes this extremely easy with the Model Tuning feature, also known as Hyperparameter Optimization (or HPO). With Model Tuning you simply decide which of the hyperparameters you are not sure about and specify the range of values for each that SageMaker needs to explore. Let's see again how this can be accomplished via the console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='cleanup'></a>\n",
    "## Cleanup\n",
    "\n",
    "At the end of the lab we would like to delete the real-time endpoint, as keeping a real-time endpoint around while being idle is costly and wasteful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
